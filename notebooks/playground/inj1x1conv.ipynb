{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Injective flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam2392/miniforge3/envs/ciflows/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from ciflows.flows import Injective1x1Conv\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import scipy\n",
    "\n",
    "\n",
    "class invertible_1x1_conv(layers.Layer):\n",
    "    \"\"\"Invertible 1x1 convolutional layers\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(invertible_1x1_conv, self).__init__()\n",
    "        self.type = kwargs.get('op_type', 'bijective')\n",
    "        self.gamma = kwargs.get('gamma', 0.0)\n",
    "        self.activation = kwargs.get('activation', 'linear')\n",
    "        \n",
    "    def build(self, input_shape, w=None):\n",
    "        _, height, width, channels = input_shape\n",
    "        \n",
    "        if self.type=='bijective':\n",
    "            random_matrix = np.random.randn(channels, channels).astype(\"float32\")\n",
    "            np_w = scipy.linalg.qr(random_matrix)[0].astype(\"float32\")\n",
    "            self.activation = 'linear'\n",
    "            \n",
    "        else:\n",
    "            if self.activation == 'linear':\n",
    "                random_matrix_1 = np.random.randn(channels//2, channels//2).astype(\"float32\")\n",
    "                random_matrix_2 = np.random.randn(channels//2, channels//2).astype(\"float32\")\n",
    "                np_w_1 = scipy.linalg.qr(random_matrix_1)[0].astype(\"float32\")\n",
    "                np_w_2 = scipy.linalg.qr(random_matrix_2)[0].astype(\"float32\")\n",
    "                np_w = np.concatenate([np_w_1, np_w_2], axis=0)/(np.sqrt(2.0))\n",
    "                \n",
    "            elif self.activation == 'relu':\n",
    "                random_matrix_1 = np.random.randn(channels//2, channels//2).astype(\"float32\")\n",
    "                np_w = scipy.linalg.qr(random_matrix_1)[0].astype(\"float32\")\n",
    "        \n",
    "        if w is not None:\n",
    "            np_w = w\n",
    "        self.w = tf.Variable(np_w, name='W', trainable=True)\n",
    "\n",
    "\n",
    "    def call(self, x, reverse=False):\n",
    "        # If height or width cannot be statically determined then they end up as\n",
    "        # tf.int32 tensors, which cannot be directly multiplied with a floating\n",
    "        # point tensor without a cast.\n",
    "        _, height, width, channels = x.get_shape().as_list()\n",
    "        s = tf.linalg.svd(self.w, \n",
    "            full_matrices=False, compute_uv=False)\n",
    "        \n",
    "        \n",
    "        log_s = tf.math.log(s + self.gamma**2/(s + 1e-8))\n",
    "        objective = tf.reduce_sum(log_s) * \\\n",
    "            tf.cast(height * width, log_s.dtype)\n",
    "    \n",
    "        if not reverse:\n",
    "            \n",
    "            if self.activation == 'relu':\n",
    "                x = x[:,:,:,:channels//2] - x[:,:,:,channels//2:]\n",
    "            w = tf.reshape(self.w , [1, 1] + self.w.get_shape().as_list())\n",
    "            print('forward pass: ', x.shape, w.shape)\n",
    "            x = tf.nn.conv2d(x, w, [1, 1, 1, 1], \"SAME\", data_format=\"NHWC\")\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if self.activation=='relu':\n",
    "                prefactor = tf.matmul(self.w, self.w, transpose_a=True) + \\\n",
    "                    self.gamma**2*tf.eye(tf.shape(self.w)[1])\n",
    "                \n",
    "                w_inv = tf.matmul(tf.linalg.inv(prefactor), self.w, transpose_b=True)\n",
    "                conv_filter = tf.concat([w_inv, -w_inv], axis=1)\n",
    "                conv_filter = tf.reshape(conv_filter, [1, 1] + conv_filter.get_shape().as_list())\n",
    "                x = tf.nn.conv2d(x, conv_filter, [1, 1, 1, 1], \"SAME\", data_format=\"NHWC\")\n",
    "                x = tf.nn.relu(x)\n",
    "            \n",
    "            else:\n",
    "                prefactor = tf.matmul(self.w, self.w, transpose_a=True) + \\\n",
    "                    self.gamma**2*tf.eye(tf.shape(self.w)[1])\n",
    "\n",
    "                w_inv = tf.matmul(  tf.linalg.inv(prefactor) , self.w, transpose_b=True)\n",
    "                print('reverse pass: ', w_inv.shape)\n",
    "                conv_filter = w_inv\n",
    "                conv_filter = tf.reshape(conv_filter, [1, 1] + conv_filter.get_shape().as_list())\n",
    "                x = tf.nn.conv2d(x, conv_filter, [1, 1, 1, 1], \"SAME\", data_format=\"NHWC\")\n",
    "            \n",
    "            objective *= -1\n",
    "        return x, objective\n",
    "\n",
    "    def jvp(self, x):\n",
    "        \"\"\"Calculates the jacobian-vector product with reverse=True\"\"\"\n",
    "        # here J is a diagonal matrix and therefore, v^T J or Jv will \n",
    "        # be the same, except the transpose. \n",
    "\n",
    "        return self.call(x, reverse=True)\n",
    "\n",
    "    def vjp(self, v):\n",
    "        \"\"\"Calculates the vector-Jacobian product with reverse=True\"\"\"\n",
    "        ## here we just transpose the pseudo-inverted filter and apply to v\n",
    "        if self.activation=='relu':\n",
    "            prefactor = tf.matmul(self.w, self.w, transpose_a=True) + \\\n",
    "                self.gamma**2*tf.eye(tf.shape(self.w)[1])\n",
    "            \n",
    "            w_inv = tf.transpose(tf.matmul(tf.linalg.inv(prefactor), self.w, transpose_b=True))\n",
    "            conv_filter = tf.concat([w_inv, -w_inv], axis=1)\n",
    "            conv_filter = tf.reshape(conv_filter, [1, 1] + conv_filter.get_shape().as_list())\n",
    "            out = tf.nn.conv2d(v, conv_filter, [1, 1, 1, 1], \"SAME\", data_format=\"NHWC\")\n",
    "            out = tf.nn.relu(out)\n",
    "        \n",
    "        else:\n",
    "            prefactor = tf.matmul(self.w, self.w, transpose_a=True) + \\\n",
    "                self.gamma**2*tf.eye(tf.shape(self.w)[1])\n",
    "\n",
    "            w_inv = tf.transpose(tf.matmul(tf.linalg.inv(prefactor) , self.w, transpose_b=True))\n",
    "            conv_filter = w_inv\n",
    "            conv_filter = tf.reshape(conv_filter, [1, 1] + conv_filter.get_shape().as_list())\n",
    "            out = tf.nn.conv2d(out, conv_filter, [1, 1, 1, 1], \"SAME\", data_format=\"NHWC\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:  (1, 32, 32, 4) (1, 1, 4, 2)\n",
      "reverse pass:  (2, 4)\n",
      "Input Tensor Shape: (1, 32, 32, 4)\n",
      "Output Tensor Shape: (1, 32, 32, 2)\n",
      "Reconstructed Tensor Shape: (1, 32, 32, 4)\n",
      "Is reconstruction close to input? False\n",
      "tf.Tensor(0.5807822, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a sample input tensor (e.g., batch size of 1, 4 channels, 32x32 image)\n",
    "input_tensor = tf.random.normal((1, 32, 32, 4))\n",
    "\n",
    "# Instantiate the layer\n",
    "invertible_layer = invertible_1x1_conv(op_type='injective', gamma=0.0, activation='linear')\n",
    "\n",
    "# Build the layer to initialize weights\n",
    "invertible_layer.build(input_tensor.shape)\n",
    "\n",
    "# Perform a forward pass\n",
    "output_tensor, objective = invertible_layer(input_tensor)\n",
    "\n",
    "# Perform a reverse pass\n",
    "reconstructed_tensor, _ = invertible_layer(output_tensor, reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "print(\"Output Tensor Shape:\", output_tensor.shape)\n",
    "print(\"Reconstructed Tensor Shape:\", reconstructed_tensor.shape)\n",
    "\n",
    "# Check if the reconstruction is close to the original input\n",
    "print(\"Is reconstruction close to input?\", tf.reduce_all(tf.abs(input_tensor - reconstructed_tensor) < 1e-5).numpy())\n",
    "print(tf.reduce_mean(tf.abs(input_tensor - reconstructed_tensor)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reverse pass:  (2, 4)\n",
      "forward pass:  (1, 32, 32, 4) (1, 1, 4, 2)\n",
      "Input Tensor Shape: (1, 32, 32, 2)\n",
      "Output Tensor Shape: (1, 32, 32, 4)\n",
      "Reconstructed Tensor Shape: (1, 32, 32, 2)\n",
      "Is reconstruction close to input? True\n",
      "tf.Tensor(3.7361964e-08, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# now we will start from the latent space and try to reconstruct the image\n",
    "input_tensor = tf.random.normal((1, 32, 32, 2))\n",
    "\n",
    "# Perform a forward pass\n",
    "output_tensor, objective = invertible_layer(input_tensor, reverse=True)\n",
    "\n",
    "# Perform a reverse pass\n",
    "reconstructed_tensor, _ = invertible_layer(output_tensor, reverse=False)\n",
    "\n",
    "# Print the results\n",
    "print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "print(\"Output Tensor Shape:\", output_tensor.shape)\n",
    "print(\"Reconstructed Tensor Shape:\", reconstructed_tensor.shape)\n",
    "\n",
    "# Check if the reconstruction is close to the original input\n",
    "print(\"Is reconstruction close to input?\", tf.reduce_all(tf.abs(input_tensor - reconstructed_tensor) < 1e-5).numpy())\n",
    "print(tf.reduce_mean(tf.abs(input_tensor - reconstructed_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import scipy.linalg\n",
    "\n",
    "\n",
    "class Invertible1x1Conv(nn.Module):\n",
    "    \"\"\"Invertible 1x1 convolutional layers\"\"\"\n",
    "\n",
    "    def __init__(self, op_type=\"bijective\", gamma=0.0, activation=\"linear\"):\n",
    "        super(Invertible1x1Conv, self).__init__()\n",
    "        self.op = op_type\n",
    "        self.gamma = gamma\n",
    "        self.activation = activation\n",
    "        self.w = None  # Placeholder for weight matrix\n",
    "\n",
    "    def build(self, channels, w=None):\n",
    "        if self.op == \"bijective\":\n",
    "            random_matrix = np.random.randn(channels, channels).astype(\"float32\")\n",
    "            np_w = scipy.linalg.qr(random_matrix)[0].astype(\"float32\")\n",
    "\n",
    "        else:  # injective operation\n",
    "            if self.activation == \"linear\":\n",
    "                random_matrix_1 = np.random.randn(channels // 2, channels // 2).astype(\n",
    "                    \"float32\"\n",
    "                )\n",
    "                np_w_1 = scipy.linalg.qr(random_matrix_1)[0].astype(\"float32\")\n",
    "                random_matrix_2 = np.random.randn(channels // 2, channels // 2).astype(\n",
    "                    \"float32\"\n",
    "                )\n",
    "                np_w_2 = scipy.linalg.qr(random_matrix_2)[0].astype(\"float32\")\n",
    "                np_w = np.concatenate([np_w_1, np_w_2], axis=1) / (np.sqrt(2.0))\n",
    "            elif self.activation == \"relu\":\n",
    "                random_matrix_1 = np.random.randn(channels // 2, channels // 2).astype(\n",
    "                    \"float32\"\n",
    "                )\n",
    "                np_w = scipy.linalg.qr(random_matrix_1)[0].astype(\"float32\")\n",
    "\n",
    "        if w is not None:\n",
    "            np_w = w\n",
    "\n",
    "        self.w = nn.Parameter(\n",
    "            torch.tensor(np_w, dtype=torch.float32), requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        # XXX: Matches now tensorflow implementation forward and reverse.\n",
    "        _, channels, height, width = x.size()\n",
    "        s = torch.svd(self.w).S\n",
    "\n",
    "        log_s = torch.log(s + self.gamma**2 / (s + 1e-8))\n",
    "        objective = torch.sum(log_s) * (height * width)\n",
    "\n",
    "        if not reverse:\n",
    "            if self.activation == \"relu\":\n",
    "                # For injective, we are combining halves\n",
    "                x_a = x[:, : channels // 2, :, :]\n",
    "                x_b = x[:, channels // 2 :, :, :]\n",
    "                x = x_a - x_b\n",
    "\n",
    "            w = self.w.view(self.w.shape[0], self.w.shape[1], 1, 1)\n",
    "            print('Forward: ', x.shape, w.shape)\n",
    "            x = F.conv2d(x, w, stride=1, padding=0)\n",
    "\n",
    "        else:\n",
    "            prefactor = torch.matmul(\n",
    "                self.w, self.w.T\n",
    "            ) + self.gamma**2 * torch.eye(self.w.shape[0])\n",
    "            w_inv = torch.matmul(self.w.T, torch.linalg.inv(prefactor))\n",
    "            if self.activation == \"relu\":\n",
    "                # prefactor = torch.mm(self.w.t(), self.w) + self.gamma**2 * torch.eye(\n",
    "                #     self.w.size(1)\n",
    "                # )\n",
    "                # w_inv = torch.mm(torch.linalg.inv(prefactor), self.w.t())\n",
    "                conv_filter = torch.cat([w_inv, -w_inv], dim=0)\n",
    "                conv_filter = conv_filter.view(\n",
    "                     conv_filter.size(0), conv_filter.size(1), 1, 1,\n",
    "                )\n",
    "                x = F.conv2d(x, conv_filter, stride=1, padding=0)\n",
    "                x = F.relu(x)\n",
    "\n",
    "            else:\n",
    "                # prefactor = torch.matmul(\n",
    "                #     self.w, self.w.T\n",
    "                # ) + self.gamma**2 * torch.eye(self.w.shape[0])\n",
    "                # w_inv = torch.matmul(self.w.T, torch.linalg.inv(prefactor))\n",
    "                print('pt reverse: ', w_inv.shape)\n",
    "                conv_filter = w_inv.view(w_inv.size(0), w_inv.size(1), 1, 1)\n",
    "                x = F.conv2d(x, conv_filter, stride=1, padding=0)\n",
    "\n",
    "            objective *= -1\n",
    "\n",
    "        return x, objective\n",
    "\n",
    "    def jvp(self, v):\n",
    "        \"\"\"Calculates the Jacobian-vector product with reverse=True\"\"\"\n",
    "        return self.forward(v, reverse=True)\n",
    "\n",
    "    def vjp(self, v):\n",
    "        \"\"\"Calculates the vector-Jacobian product with reverse=True\"\"\"\n",
    "        prefactor = torch.matmul(\n",
    "            self.w, self.w.T\n",
    "        ) + self.gamma**2 * torch.eye(self.w.shape[0])\n",
    "        w_inv = torch.matmul(self.w.T, torch.linalg.inv(prefactor))\n",
    "        if self.activation == \"relu\":\n",
    "            # prefactor = torch.mm(self.w.t(), self.w) + self.gamma**2 * torch.eye(\n",
    "            #     self.w.size(1)\n",
    "            # )\n",
    "            # w_inv = self.w.t() @ torch.linalg.inv(prefactor)\n",
    "            conv_filter = torch.cat([w_inv, -w_inv], dim=0)\n",
    "            conv_filter = conv_filter.view(\n",
    "                conv_filter.size(0), conv_filter.size(1), 1, 1\n",
    "            )\n",
    "            out = F.conv2d(v, conv_filter, stride=1, padding=0)\n",
    "            out = F.relu(out)\n",
    "\n",
    "        else:\n",
    "            # prefactor = torch.mm(self.w.t(), self.w) + self.gamma**2 * torch.eye(\n",
    "            #     self.w.size(1)\n",
    "            # )\n",
    "            # w_inv = self.w.t() @ torch.linalg.inv(prefactor)\n",
    "            conv_filter = w_inv.view(w_inv.size(0), w_inv.size(1), 1, 1)\n",
    "            out = F.conv2d(v, conv_filter, stride=1, padding=0)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4]) (4, 2)\n",
      "tensor([[ 0.5000, -0.0000, -0.1550, -0.4750],\n",
      "        [-0.0000,  0.5000,  0.4750, -0.1550],\n",
      "        [-0.1550,  0.4750,  0.5000, -0.0000],\n",
      "        [-0.4750, -0.1550, -0.0000,  0.5000]])\n",
      "tensor([[ 0.5000,  0.0000,  0.4960, -0.0590],\n",
      "        [ 0.0000,  0.5000,  0.0590,  0.4960],\n",
      "        [ 0.4960,  0.0590,  0.5000, -0.0000],\n",
      "        [-0.0590,  0.4960, -0.0000,  0.5000]])\n"
     ]
    }
   ],
   "source": [
    "# test the w matrix setup:\n",
    "\n",
    "your_channel_count = 4\n",
    "batch_size = 2\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "# Create a sample input tensor (e.g., batch size of 1, 4 channels, 32x32 image)\n",
    "input_tensor = tf.random.normal((1, 28, 28, 4))\n",
    "invertible_layer = invertible_1x1_conv(op_type='injective', gamma=0.0, activation='linear')\n",
    "invertible_layer.build(input_tensor.shape)\n",
    "\n",
    "conv_layer = Invertible1x1Conv(op_type=\"injective\", activation=\"linear\")\n",
    "conv_layer.build(channels=your_channel_count)\n",
    "\n",
    "print(conv_layer.w.shape, invertible_layer.w.shape)\n",
    "print(np.round((conv_layer.w.T @ conv_layer.w).detach(), 3))\n",
    "print(np.round((torch.Tensor(np.array(invertible_layer.w)) @ torch.Tensor(np.array(invertible_layer.w)).T).detach(), 3))\n",
    "\n",
    "# assert torch.allclose(conv_layer.w, torch.Tensor(np.array(invertible_layer.w))), \"Weight matrix is not symmetric\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape for TF:  (4, 2)\n",
      "forward pass:  (1, 2, 2, 2) (1, 1, 2, 2)\n",
      "MSE:  tf.Tensor(1.0731158, shape=(), dtype=float32)\n",
      "W shape for PT:  (2, 2)\n",
      "Forward:  torch.Size([1, 2, 2, 2]) torch.Size([2, 2, 1, 1])\n",
      "torch.Size([1, 4, 2, 2]) torch.Size([1, 2, 2, 2]) torch.Size([1, 4, 2, 2])\n",
      "MSE:  tensor(1.0731, grad_fn=<MseLossBackward0>)\n",
      "tensor(0., grad_fn=<MseLossBackward0>)\n",
      "tensor(0., grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "activation = 'relu'\n",
    "\n",
    "# Create a sample input tensor (e.g., batch size of 1, 4 channels, 32x32 image)\n",
    "input_tensor = tf.random.normal((1, 2, 2, 4))\n",
    "input_tensor_pt = torch.Tensor(np.array(input_tensor)).permute(0, 3, 1, 2)  # NHWC to NCHW\n",
    "\n",
    "channels = input_tensor.shape[-1]\n",
    "random_matrix_1 = np.random.randn(channels // 2, channels // 2).astype(\"float32\")\n",
    "np_w_1 = scipy.linalg.qr(random_matrix_1)[0].astype(\"float32\")\n",
    "random_matrix_2 = np.random.randn(channels // 2, channels // 2).astype(\"float32\")\n",
    "np_w_2 = scipy.linalg.qr(random_matrix_2)[0].astype(\"float32\")\n",
    "np_w = np.concatenate([np_w_1, np_w_2], axis=0) / (np.sqrt(2.0))\n",
    "print('W shape for TF: ', np_w.shape)\n",
    "if activation == \"relu\":\n",
    "    random_matrix_1 = np.random.randn(channels // 2, channels // 2).astype(\"float32\")\n",
    "    np_w = scipy.linalg.qr(random_matrix_1)[0].astype(\"float32\")\n",
    "\n",
    "# test tensorflow\n",
    "invertible_layer = invertible_1x1_conv(op_type='injective', gamma=0.0, activation=activation)\n",
    "invertible_layer.build(input_tensor.shape, w=np_w)\n",
    "output_tensor, objective = invertible_layer(input_tensor)\n",
    "# print(output_tensor.shape)\n",
    "recon_tensor, objective = invertible_layer(output_tensor, reverse=True)\n",
    "\n",
    "# Create MSE metric\n",
    "mse_metric = tf.keras.metrics.MeanSquaredError()\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mse_metric(input_tensor, recon_tensor)\n",
    "print('MSE: ', mse)\n",
    "\n",
    "# test pytorch\n",
    "if activation == 'linear':\n",
    "    np_w = np.concatenate([np_w_1, np_w_2], axis=1) / (np.sqrt(2.0))\n",
    "print('W shape for PT: ', np_w.shape)\n",
    "conv_layer = Invertible1x1Conv(op_type=\"injective\", activation=activation)\n",
    "conv_layer.build(channels=your_channel_count, w=np_w)\n",
    "output_tensor_pt, objective_pt = conv_layer(input_tensor_pt)\n",
    "\n",
    "recon_tensor_pt, objective_pt = conv_layer(output_tensor_pt, reverse=True)\n",
    "\n",
    "# compute mse\n",
    "print(input_tensor_pt.shape, output_tensor_pt.shape, recon_tensor_pt.shape)\n",
    "\n",
    "mse = torch.nn.functional.mse_loss(input_tensor_pt, recon_tensor_pt)\n",
    "print('MSE: ', mse)\n",
    "\n",
    "\n",
    "# mse = torch.nn.functional.mse_loss(input_tensor_pt, torch.Tensor(np.array(recon_tensor)).permute(0, 3, 1, 2))\n",
    "# print(f'MSE: ', mse)\n",
    "\n",
    "print(torch.nn.functional.mse_loss(recon_tensor_pt, torch.Tensor(np.array(recon_tensor)).permute(0, 3, 1, 2)))\n",
    "print(torch.nn.functional.mse_loss(output_tensor_pt, torch.Tensor(np.array(output_tensor)).permute(0, 3, 1, 2)))\n",
    "# np.testing.assert_approx_equal(objective_pt.detach().numpy(), np.array(objective), significant=5), \"Objective values are not equal\"\n",
    "# print(output_tensor, output_tensor_pt)\n",
    "# print(output_tensor.shape, output_tensor_pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0573,  0.9984],\n",
      "        [ 0.9984,  0.0573]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(conv_layer.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 2, 2]) torch.Size([1, 2, 2, 2]) torch.Size([1, 4, 2, 2])\n",
      "MSE:  tensor(0., grad_fn=<MseLossBackward0>)\n",
      "MSE:  tensor(0., grad_fn=<MseLossBackward0>)\n",
      "torch.Size([1, 4, 2, 2]) torch.Size([1, 2, 2, 2]) torch.Size([1, 4, 2, 2])\n",
      "MSE:  tensor(1.0731, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test pytorch glow\n",
    "flow = Injective1x1Conv(num_channels_in=your_channel_count, activation='relu', preset_W=torch.Tensor(np_w))\n",
    "output_tensor_pt, objective_pt = flow.inverse(input_tensor_pt)\n",
    "recon_tensor_pt, objective_pt = flow.forward(output_tensor_pt)\n",
    "\n",
    "# compute mse\n",
    "print(input_tensor_pt.shape, output_tensor_pt.shape, recon_tensor_pt.shape)\n",
    "\n",
    "mse = torch.nn.functional.mse_loss(output_tensor_pt, torch.Tensor(np.array(output_tensor)).permute(0, 3, 1, 2))\n",
    "print('MSE: ', mse)\n",
    "mse = torch.nn.functional.mse_loss(recon_tensor_pt, torch.Tensor(np.array(recon_tensor)).permute(0, 3, 1, 2))\n",
    "print('MSE: ', mse)\n",
    "\n",
    "print(input_tensor_pt.shape, output_tensor_pt.shape, recon_tensor_pt.shape)\n",
    "mse = torch.nn.functional.mse_loss(input_tensor_pt, recon_tensor_pt)\n",
    "print('MSE: ', mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000e+00, -1.3758e-07],\n",
      "        [-1.3758e-07,  1.0000e+00]], grad_fn=<AddBackward0>)\n",
      "pt reverse:  torch.Size([4, 2]) tensor([[-0.1462,  0.6918],\n",
      "        [ 0.6918,  0.1462],\n",
      "        [-0.2179, -0.6727],\n",
      "        [-0.6727,  0.2179]], grad_fn=<MmBackward0>)\n",
      "tf.Tensor(\n",
      "[[ 1.0000001e+00 -1.3758199e-07]\n",
      " [-1.3758199e-07  1.0000000e+00]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.14615329  0.69183755 -0.21789008 -0.67269886]\n",
      " [ 0.6918376   0.14615338 -0.67269903  0.21789002]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "prefactor = torch.matmul(\n",
    "    conv_layer.w, conv_layer.w.T\n",
    ") + conv_layer.gamma**2 * torch.eye(conv_layer.w.shape[0])\n",
    "print(prefactor)\n",
    "w_inv_pt = torch.matmul(conv_layer.w.T, torch.linalg.inv(prefactor))\n",
    "print('pt reverse: ', w_inv_pt.shape, w_inv_pt)\n",
    "\n",
    "prefactor = tf.matmul(\n",
    "    invertible_layer.w, invertible_layer.w, transpose_a=True\n",
    ") + invertible_layer.gamma**2 * torch.eye(invertible_layer.w.shape[1])\n",
    "print(prefactor)\n",
    "w_inv = tf.matmul(  tf.linalg.inv(prefactor) , invertible_layer.w, transpose_b=True)\n",
    "print(w_inv)\n",
    "\n",
    "assert torch.allclose(w_inv_pt.T, torch.Tensor(np.array(w_inv))), \"Weight matrix same\"\n",
    "\n",
    "\n",
    "# w_inv = torch.matmul(conv_layer.w, torch.linalg.inv(prefactor), conv_layer.w)\n",
    "# print('pt reverse: ', w_inv.shape)\n",
    "# conv_filter = w_inv.view(w_inv.size(0), w_inv.size(1), 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your_channel_count = 4\n",
    "# batch_size = 2\n",
    "# height = 28\n",
    "# width = 28\n",
    "\n",
    "# conv_layer = Invertible1x1Conv(op_type=\"injective\", activation=\"linear\")\n",
    "# conv_layer.build(channels=your_channel_count)\n",
    "\n",
    "# # Test with random input -> latents\n",
    "# x = torch.randn(batch_size, your_channel_count, height, width)\n",
    "# output, objective = conv_layer.forward(x, reverse=False)\n",
    "\n",
    "# # reverse the latents to get the inputs\n",
    "# x_reversed, objective_reversed = conv_layer.forward(output, reverse=False)\n",
    "\n",
    "# assert torch.allclose(x, x_reversed)\n",
    "# print(output.shape, x.shape, objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ciflows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
