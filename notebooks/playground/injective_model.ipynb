{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Injective model playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam2392/miniforge3/envs/ciflows/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import lightning as pl\n",
    "import normflows as nf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from ciflows.datasets.lightning import MultiDistrDataModule, DatasetName\n",
    "from ciflows.distributions.linear import ClusteredLinearGaussianDistribution\n",
    "from ciflows.flows import TwoStageTraining, plCausalInjFlowModel\n",
    "from ciflows.flows.glow import (GlowBlock, InjectiveGlowBlock, ReshapeFlow,\n",
    "                                Squeeze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (3, 64, 64)\n",
    "\n",
    "def get_inj_model(input_shape):\n",
    "    use_lu = True\n",
    "    gamma = 1e-6\n",
    "    activation = \"linear\"\n",
    "    dropout_probability = 0.2\n",
    "\n",
    "    net_actnorm = False\n",
    "    # n_hidden_list = [32, 64, 128, 256, 256, 256]\n",
    "    n_hidden = 32\n",
    "    n_glow_blocks = 3\n",
    "    n_mixing_layers = 4\n",
    "    n_injective_layers = 8\n",
    "    n_layers = n_mixing_layers + n_injective_layers\n",
    "\n",
    "    # hidden layers for the AutoregressiveRationalQuadraticSpline\n",
    "    net_hidden_layers = 2\n",
    "    net_hidden_dim = 32\n",
    "\n",
    "    n_channels = input_shape[0]\n",
    "    img_size = input_shape[1]\n",
    "\n",
    "    n_chs = n_channels\n",
    "    flows = []\n",
    "\n",
    "    debug = False\n",
    "\n",
    "    n_chs = int(n_channels * 4**n_mixing_layers * (1 / 2) ** n_injective_layers)\n",
    "    latent_size = int(img_size / (2**n_mixing_layers))\n",
    "    init_n_chs = n_chs\n",
    "    init_latent_size = latent_size\n",
    "    print(\n",
    "        \"Starting at latent representation: \", n_chs, \"with latent size: \", latent_size\n",
    "    )\n",
    "    q0 = nf.distributions.DiagGaussian(\n",
    "        (n_chs, latent_size, latent_size), trainable=False\n",
    "    )\n",
    "\n",
    "    split_mode = \"channel\"\n",
    "\n",
    "    for i in range(n_injective_layers):\n",
    "        # n_hidden = n_hidden_list[-i]\n",
    "        if i <= 1:\n",
    "            split_mode = \"checkerboard\"\n",
    "        else:\n",
    "            split_mode = \"channel\"\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            for j in range(n_glow_blocks):\n",
    "                flows += [\n",
    "                    GlowBlock(\n",
    "                        channels=n_chs,\n",
    "                        hidden_channels=n_hidden,\n",
    "                        use_lu=use_lu,\n",
    "                        scale=True,\n",
    "                        split_mode=split_mode,\n",
    "                        net_actnorm=net_actnorm,\n",
    "                        dropout_probability=dropout_probability,\n",
    "                    )\n",
    "                ]\n",
    "        else:\n",
    "            flows += [\n",
    "                ReshapeFlow(\n",
    "                    shape_in=(n_chs, latent_size, latent_size),\n",
    "                    shape_out=(n_chs * latent_size * latent_size,),\n",
    "                )\n",
    "            ]\n",
    "            flows += [\n",
    "                nf.flows.AutoregressiveRationalQuadraticSpline(\n",
    "                    num_input_channels=n_chs * latent_size * latent_size,\n",
    "                    num_blocks=net_hidden_layers,\n",
    "                    num_hidden_channels=net_hidden_dim,\n",
    "                    permute_mask=True,\n",
    "                )\n",
    "            ]\n",
    "            flows += [\n",
    "                ReshapeFlow(\n",
    "                    shape_in=(n_chs * latent_size * latent_size,),\n",
    "                    shape_out=(n_chs, latent_size, latent_size),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # input to inj flow is what is at the X -> V layer\n",
    "        flows += [\n",
    "            InjectiveGlowBlock(\n",
    "                channels=n_chs,\n",
    "                hidden_channels=n_hidden,\n",
    "                activation=activation,\n",
    "                scale=True,\n",
    "                gamma=gamma,\n",
    "                debug=debug,\n",
    "                split_mode=split_mode,\n",
    "                net_actnorm=net_actnorm,\n",
    "            )\n",
    "        ]\n",
    "        n_chs = n_chs * 2\n",
    "        latent_size = latent_size\n",
    "        if debug:\n",
    "            print(f\"On layer {n_layers - i}, n_chs = {n_chs//2} -> {n_chs}\")\n",
    "\n",
    "    # split_mode = \"channel_inv\"\n",
    "    for i in range(n_mixing_layers):\n",
    "        # if i > 0:# n_mixing_layers - 1:\n",
    "        for j in range(n_glow_blocks):\n",
    "            flows += [\n",
    "                GlowBlock(\n",
    "                    channels=n_chs,\n",
    "                    hidden_channels=n_hidden,\n",
    "                    use_lu=use_lu,\n",
    "                    scale=False,\n",
    "                    split_mode=split_mode,\n",
    "                    dropout_probability=dropout_probability,\n",
    "                )\n",
    "            ]\n",
    "        # else:\n",
    "        #     flows += [\n",
    "        #         ReshapeFlow(\n",
    "        #             shape_in=(n_chs, latent_size, latent_size),\n",
    "        #             shape_out=(n_chs * latent_size * latent_size,),\n",
    "        #         )\n",
    "        #     ]\n",
    "        #     flows += [\n",
    "        #         nf.flows.AutoregressiveRationalQuadraticSpline(\n",
    "        #             num_input_channels=n_chs * latent_size * latent_size,\n",
    "        #             num_blocks=net_hidden_layers,\n",
    "        #             num_hidden_channels=net_hidden_dim,\n",
    "        #             permute_mask=True,\n",
    "        #         )\n",
    "        #     ]\n",
    "        #     flows += [\n",
    "        #         ReshapeFlow(\n",
    "        #             shape_in=(n_chs * latent_size * latent_size,),\n",
    "        #             shape_out=(n_chs, latent_size, latent_size),\n",
    "        #         )\n",
    "        #     ]\n",
    "\n",
    "        flows += [Squeeze()]\n",
    "        n_chs = n_chs // 4\n",
    "        latent_size *= 2\n",
    "        if debug:\n",
    "            print(f\"On layer {n_mixing_layers - i}, n_chs = {n_chs}\")\n",
    "\n",
    "    model = nf.NormalizingFlow(q0=q0, flows=flows)\n",
    "    model.output_n_chs = init_n_chs\n",
    "    model.output_latent_size = init_latent_size\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(pytorch_total_params)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_bij_model(\n",
    "    n_chs,\n",
    "    latent_size,\n",
    "    adj_mat,\n",
    "    cluster_sizes,\n",
    "    intervention_targets,\n",
    "    confounded_variables,\n",
    "):\n",
    "    use_lu = True\n",
    "    net_actnorm = False\n",
    "    n_hidden = 128\n",
    "    n_glow_blocks = 6\n",
    "\n",
    "    flows = []\n",
    "\n",
    "    debug = False\n",
    "\n",
    "    print(\"Starting at latent representation: \", n_chs, latent_size, latent_size)\n",
    "    # q0 = nf.distributions.DiagGaussian(\n",
    "    #     (n_chs, latent_size, latent_size), trainable=False\n",
    "    # )\n",
    "    q0 = nf.distributions.DiagGaussian(\n",
    "        (n_chs * latent_size * latent_size,), trainable=False\n",
    "    )\n",
    "\n",
    "    # q0 = ClusteredLinearGaussianDistribution(\n",
    "    #     adjacency_matrix=adj_mat,\n",
    "    #     cluster_sizes=cluster_sizes,\n",
    "    #     intervention_targets_per_distr=intervention_targets,\n",
    "    #     hard_interventions_per_distr=None,\n",
    "    #     confounded_variables=confounded_variables,\n",
    "    # )\n",
    "\n",
    "    split_mode = \"checkerboard\"\n",
    "\n",
    "    net_hidden_layers = 2\n",
    "    net_hidden_dim = 32\n",
    "\n",
    "    # flows += [\n",
    "    #     ReshapeFlow(\n",
    "    #         shape_in=(n_chs, latent_size, latent_size),\n",
    "    #         shape_out=(n_chs * latent_size * latent_size,),\n",
    "    #     )\n",
    "    # ]\n",
    "    # n_chs = n_chs * latent_size * latent_size\n",
    "\n",
    "    # using glow blocks\n",
    "    # n_chs = int(n_chs * 4**n_glow_blocks)\n",
    "    for i in range(n_glow_blocks):\n",
    "        # Neural network with two hidden layers having 64 units each\n",
    "        # Last layer is initialized by zeros making training more stable\n",
    "        # n_chs *= 4\n",
    "        # param_map = nf.nets.MLP([n_chs, net_hidden_dim, n_chs*2], init_zeros=True)\n",
    "        # # # Add flow layer\n",
    "        # flows.append(nf.flows.AffineCouplingBlock(param_map, split_mode='channel'))\n",
    "        # flows.append(nf.flows.Permute(n_chs, mode='swap'))\n",
    "\n",
    "        # Autoregressive Neural Spline flow\n",
    "        # Swap dimensions\n",
    "        flows += [\n",
    "            nf.flows.AutoregressiveRationalQuadraticSpline(\n",
    "                num_input_channels=n_chs * latent_size * latent_size,\n",
    "                num_blocks=net_hidden_layers,\n",
    "                num_hidden_channels=net_hidden_dim,\n",
    "                permute_mask=True,\n",
    "            )\n",
    "        ]\n",
    "        if debug:\n",
    "            print(f\"On layer {n_glow_blocks - i}, n_chs = {n_chs//2} -> {n_chs}\")\n",
    "\n",
    "    # maps x to (n_chs * latent_size * latent_size), while v is mapped to (n_chs, latent_size, latent_size)\n",
    "    flows += [\n",
    "        ReshapeFlow(\n",
    "            shape_in=(n_chs * latent_size * latent_size,),\n",
    "            shape_out=(n_chs, latent_size, latent_size),\n",
    "        )\n",
    "    ]\n",
    "    model = nf.NormalizingFlow(q0=q0, flows=flows)\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(pytorch_total_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at latent representation:  3 with latent size:  4\n",
      "17151759\n",
      "3 4\n"
     ]
    }
   ],
   "source": [
    "inj_model = get_inj_model(input_shape)\n",
    "init_chs = inj_model.output_n_chs\n",
    "init_latent_size = inj_model.output_latent_size\n",
    "print(init_chs, init_latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at latent representation:  3 4 4\n",
      "253344\n"
     ]
    }
   ],
   "source": [
    "bij_model = get_bij_model(init_chs, init_latent_size, None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64])\n",
      "torch.Size([2, 3, 64, 64])\n",
      "torch.Size([2, 3, 4, 4])\n",
      "torch.Size([2, 3, 4, 4])\n",
      "torch.Size([2, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test_sample, _ = inj_model.sample(2)\n",
    "print(test_sample.shape)\n",
    "\n",
    "test_tensor = torch.randn(2, *input_shape)\n",
    "print(test_tensor.shape)\n",
    "test_sample = inj_model.inverse(test_tensor)\n",
    "print(test_sample.shape)\n",
    "\n",
    "test_latent_tensor = torch.randn(2, init_chs, init_latent_size, init_latent_size)\n",
    "print(test_latent_tensor.shape)\n",
    "test_sample = inj_model.forward(test_latent_tensor)\n",
    "print(test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 48])\n",
      "torch.Size([2, 48])\n",
      "torch.Size([2, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test_tensor = torch.randn(2, *input_shape)\n",
    "test_sample = bij_model.inverse(inj_model.inverse(test_tensor))\n",
    "print(test_sample.shape)\n",
    "\n",
    "test_latent_tensor = torch.randn(2, init_chs * init_latent_size * init_latent_size)\n",
    "print(test_latent_tensor.shape)\n",
    "test_sample = inj_model.forward(bij_model.forward(test_latent_tensor))\n",
    "print(test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam2392/miniforge3/envs/ciflows/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'inj_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['inj_model'])`.\n",
      "/Users/adam2392/miniforge3/envs/ciflows/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'bij_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['bij_model'])`.\n"
     ]
    }
   ],
   "source": [
    "n_chs = inj_model.output_n_chs\n",
    "latent_size = inj_model.output_latent_size\n",
    "\n",
    "example_input_array = [\n",
    "    torch.randn(2, n_chs * latent_size * latent_size),\n",
    "    torch.randn(2, 1),\n",
    "]\n",
    "model = plCausalInjFlowModel(\n",
    "    inj_model=inj_model,\n",
    "    bij_model=bij_model,\n",
    "    example_input_array=example_input_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ciflows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
